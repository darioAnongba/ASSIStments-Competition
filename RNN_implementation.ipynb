{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASSISTments Data Mining Competition 2017 - Optional Semester Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import functools\n",
    "import tensorflow as tf\n",
    "\n",
    "# Constants\n",
    "DATA_DIR = 'Data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load all the data into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_train_logs = pd.read_pickle('student_train_logs')\n",
    "student_test_logs = pd.read_pickle('student_test_logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = student_train_logs['ITEST_id'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the training labels into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = pd.read_csv('Data/training_label.csv', index_col='ITEST_id').sort_index()\n",
    "train_labels.drop_duplicates(subset=None, keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use an RNN (LSTM), we need to create sequences of actions. A sequence is a matrix in which each row is an action taken by a student that follows a chronological order. An action is a row in the provided dataset (see above). In summary, we produce a Tensor (a 3D array) with the following dimensions:\n",
    "\n",
    "* First: All the students actions\n",
    "* Second: A list of actions for a specific student\n",
    "* Third: A specific action (which in turn is a vector of metrics (see above))\n",
    "\n",
    "The output that interests us is the last output of the sequence. Which is the output consisting of the last action of a specific student taking into account all previous actions of that student. This output will represent if the student will choose a STEM career or not.\n",
    "\n",
    "**A problem that we will encounter and will need to solve is that sequences have dynamic lengths**. Because Tensors need to be of fixed predefined size. Meaning that not all students have done the same number of actions while learning.\n",
    "\n",
    "A possible solution could be to use *padding*. Padding means normalizing the size of all sequences (to the maximum of all sequences size) and add 0 vectors for sequences that are smaller than the maximum size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of the input tensor\n",
    "train_input = []\n",
    "\n",
    "for idx in train_idx:\n",
    "    specific_student_actions = student_train_logs[student_train_logs['ITEST_id'] == idx]\n",
    "    student_actions = []\n",
    "    \n",
    "    for action_idx, action in specific_student_actions.iterrows():\n",
    "        student_actions.append(np.array(action))\n",
    "    \n",
    "    train_input.append(np.array(student_actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, first student has taken 504 actions while the second student has taken 129 actions.\n",
    "\n",
    "We retrieve the biggest sequence and increase the size of all other sequences to be equal to the maximum with 0 vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2742"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sequence_size = 0\n",
    "for i in range(len(train_input)):\n",
    "    max_sequence_size = max(max_sequence_size, len(train_input[i]))\n",
    "max_sequence_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_padd = []\n",
    "n_features = len(train_input[0][0])\n",
    "\n",
    "for i in range(len(train_input)):\n",
    "    n = len(train_input[i])\n",
    "    rem = max_sequence_size - n\n",
    "\n",
    "    z = np.zeros((rem, n_features), dtype=train_input[i].dtype)\n",
    "    train_input_padd.append(np.concatenate((train_input[i], z)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2742, 51)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input_padd[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train output data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training output is a list of one-hot vectors of size 2 (one for each student). If the value of index 0 is 1, then it is non-STEM, if value at index 1 is 1, then it is STEM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_output = []\n",
    "\n",
    "for idx, row in train_labels.iterrows():\n",
    "    temp_arr = ([0] * 2)\n",
    "    \n",
    "    if row['isSTEM'] == 1:\n",
    "        temp_arr[1] = 1\n",
    "    else:\n",
    "        temp_arr[0] = 1\n",
    "        \n",
    "    train_output.append(temp_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([350, 117])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(train_output, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will split the training data into two sets, one for training and the other for testing. We will randomly take 10% of the data for testing and use the rest as training data. Ultimately we will train with everything before predicting.\n",
    "\n",
    "**TODO** Améliorer ça et utiliser kFold cross validation (stratified), parce qu'on a relativement peu de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "NUM_TEST = int(np.ceil(len(train_input_padd) * 0.05))\n",
    "print(NUM_TEST)\n",
    "X_train = train_input_padd[NUM_TEST:]\n",
    "y_train = train_output[NUM_TEST:]\n",
    "X_test = train_input_padd[:NUM_TEST]\n",
    "y_test = train_output[:NUM_TEST]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of students for training: 443\n",
      "Number of students for testing: 24\n"
     ]
    }
   ],
   "source": [
    "print('Number of students for training:', len(X_train))\n",
    "print('Number of students for testing:', len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we fist create placeholders to hold our data. The dimensions for data are [Batch Size, Sequence Length, Input Dimension]. Here our values are:\n",
    "\n",
    "* Batch size: to be defined at runtime\n",
    "* Sequence length: Different for each student\n",
    "* Input Dimension: Known (number of features of the data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lazy_property(function):\n",
    "    attribute = '_' + function.__name__\n",
    "\n",
    "    @property\n",
    "    @functools.wraps(function)\n",
    "    def wrapper(self):\n",
    "        if not hasattr(self, attribute):\n",
    "            setattr(self, attribute, function(self))\n",
    "        return getattr(self, attribute)\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "class VariableSequenceClassification:\n",
    "\n",
    "    def __init__(self, data, target, num_hidden=70, num_layers=2):\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self._num_hidden = num_hidden\n",
    "        self._num_layers = num_layers\n",
    "        self.prediction\n",
    "        self.error\n",
    "        self.optimize\n",
    "\n",
    "    @lazy_property\n",
    "    def length(self):\n",
    "        used = tf.sign(tf.reduce_max(tf.abs(self.data), reduction_indices=2))\n",
    "        length = tf.reduce_sum(used, reduction_indices=1)\n",
    "        length = tf.cast(length, tf.int32)\n",
    "        return length\n",
    "\n",
    "    @lazy_property\n",
    "    def prediction(self):\n",
    "        # Recurrent network.\n",
    "        output, _ = tf.nn.dynamic_rnn(\n",
    "            tf.nn.rnn_cell.LSTMCell(self._num_hidden),\n",
    "            data,\n",
    "            dtype=tf.float32,\n",
    "            sequence_length=self.length,\n",
    "        )\n",
    "        last = self._last_relevant(output, self.length)\n",
    "        # Softmax layer.\n",
    "        weight, bias = self._weight_and_bias(\n",
    "            self._num_hidden, int(self.target.get_shape()[1]))\n",
    "        prediction = tf.nn.softmax(tf.matmul(last, weight) + bias)\n",
    "        return prediction\n",
    "\n",
    "    @lazy_property\n",
    "    def cost(self):\n",
    "        cross_entropy = -tf.reduce_sum(self.target * tf.log(self.prediction))\n",
    "        return cross_entropy\n",
    "\n",
    "    @lazy_property\n",
    "    def optimize(self):\n",
    "        learning_rate = 0.0003\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        return optimizer.minimize(self.cost)\n",
    "\n",
    "    @lazy_property\n",
    "    def error(self):\n",
    "        mistakes = tf.not_equal(\n",
    "            tf.argmax(self.target, 1), tf.argmax(self.prediction, 1))\n",
    "        return tf.reduce_mean(tf.cast(mistakes, tf.float32))\n",
    "\n",
    "    @staticmethod\n",
    "    def _weight_and_bias(in_size, out_size):\n",
    "        weight = tf.truncated_normal([in_size, out_size], stddev=0.01)\n",
    "        bias = tf.constant(0.1, shape=[out_size])\n",
    "        return tf.Variable(weight), tf.Variable(bias)\n",
    "\n",
    "    @staticmethod\n",
    "    def _last_relevant(output, length):\n",
    "        batch_size = tf.shape(output)[0]\n",
    "        max_length = int(output.get_shape()[1])\n",
    "        output_size = int(output.get_shape()[2])\n",
    "        index = tf.range(0, batch_size) * max_length + (length - 1)\n",
    "        flat = tf.reshape(output, [-1, output_size])\n",
    "        relevant = tf.gather(flat, index)\n",
    "        return relevant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flow execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We execute the flow execution graph defined before with our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dario/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:95: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 error 37.5%\n",
      "[[ 0.5075289   0.49247113]\n",
      " [ 0.50085479  0.49914521]\n",
      " [ 0.51198596  0.4880141 ]\n",
      " [ 0.49997818  0.50002187]\n",
      " [ 0.50667328  0.49332681]\n",
      " [ 0.51198596  0.4880141 ]\n",
      " [ 0.50492579  0.49507415]\n",
      " [ 0.5119859   0.48801413]\n",
      " [ 0.5122174   0.4877826 ]\n",
      " [ 0.49997818  0.50002187]\n",
      " [ 0.51278174  0.48721829]\n",
      " [ 0.51112705  0.48887298]\n",
      " [ 0.51112705  0.48887298]\n",
      " [ 0.49997818  0.50002187]\n",
      " [ 0.51332843  0.4866716 ]\n",
      " [ 0.50953275  0.49046725]\n",
      " [ 0.51147872  0.48852125]\n",
      " [ 0.50873464  0.49126539]\n",
      " [ 0.5087347   0.49126536]\n",
      " [ 0.51147866  0.48852128]\n",
      " [ 0.49997818  0.50002187]\n",
      " [ 0.50413603  0.495864  ]\n",
      " [ 0.51147866  0.48852128]\n",
      " [ 0.51147872  0.48852125]]\n",
      "Epoch  2 error 20.8%\n",
      "[[ 0.51706856  0.48293138]\n",
      " [ 0.51935345  0.48064655]\n",
      " [ 0.52961993  0.47038007]\n",
      " [ 0.51123428  0.48876569]\n",
      " [ 0.52701777  0.47298226]\n",
      " [ 0.52961993  0.47038007]\n",
      " [ 0.51531875  0.48468119]\n",
      " [ 0.52961987  0.4703801 ]\n",
      " [ 0.53158599  0.46841398]\n",
      " [ 0.51123428  0.48876569]\n",
      " [ 0.53279454  0.46720549]\n",
      " [ 0.52852541  0.47147462]\n",
      " [ 0.52852541  0.47147462]\n",
      " [ 0.51123428  0.48876569]\n",
      " [ 0.53343201  0.46656799]\n",
      " [ 0.51774991  0.48225009]\n",
      " [ 0.53063452  0.46936548]\n",
      " [ 0.5271377   0.4728623 ]\n",
      " [ 0.52713788  0.47286209]\n",
      " [ 0.53063434  0.46936566]\n",
      " [ 0.51123428  0.48876569]\n",
      " [ 0.51922423  0.48077583]\n",
      " [ 0.53063434  0.46936566]\n",
      " [ 0.53063452  0.46936548]]\n",
      "Epoch  3 error 20.8%\n",
      "[[ 0.52617121  0.4738287 ]\n",
      " [ 0.5373047   0.46269527]\n",
      " [ 0.54671264  0.4532873 ]\n",
      " [ 0.52238166  0.47761834]\n",
      " [ 0.54675412  0.45324588]\n",
      " [ 0.54671264  0.4532873 ]\n",
      " [ 0.5256148   0.47438526]\n",
      " [ 0.54671258  0.45328739]\n",
      " [ 0.55035305  0.44964695]\n",
      " [ 0.52238166  0.47761834]\n",
      " [ 0.55217552  0.44782448]\n",
      " [ 0.54539305  0.45460689]\n",
      " [ 0.54539305  0.45460689]\n",
      " [ 0.52238166  0.47761834]\n",
      " [ 0.55291182  0.44708818]\n",
      " [ 0.52552885  0.47447118]\n",
      " [ 0.54919904  0.45080093]\n",
      " [ 0.54498494  0.455015  ]\n",
      " [ 0.54498529  0.45501474]\n",
      " [ 0.54919875  0.45080125]\n",
      " [ 0.52238166  0.47761834]\n",
      " [ 0.53385752  0.46614251]\n",
      " [ 0.54919875  0.45080125]\n",
      " [ 0.54919904  0.45080096]]\n",
      "Epoch  4 error 20.8%\n",
      "[[ 0.53478253  0.46521744]\n",
      " [ 0.5544855   0.44551453]\n",
      " [ 0.56305283  0.43694714]\n",
      " [ 0.53334695  0.46665302]\n",
      " [ 0.56563568  0.43436429]\n",
      " [ 0.56305283  0.43694714]\n",
      " [ 0.53574914  0.46425086]\n",
      " [ 0.56305277  0.43694723]\n",
      " [ 0.56828123  0.4317188 ]\n",
      " [ 0.53334695  0.46665302]\n",
      " [ 0.57067686  0.42932314]\n",
      " [ 0.56152278  0.43847725]\n",
      " [ 0.56152278  0.43847725]\n",
      " [ 0.53334695  0.46665302]\n",
      " [ 0.57152241  0.42847764]\n",
      " [ 0.53286421  0.46713579]\n",
      " [ 0.56693882  0.43306121]\n",
      " [ 0.56205606  0.43794391]\n",
      " [ 0.56205648  0.43794346]\n",
      " [ 0.56693834  0.43306166]\n",
      " [ 0.53334695  0.46665302]\n",
      " [ 0.5478667   0.45213339]\n",
      " [ 0.56693834  0.43306166]\n",
      " [ 0.56693882  0.43306121]]\n"
     ]
    }
   ],
   "source": [
    "num_classes = 2\n",
    "row_size = n_features\n",
    "batch_size = 50\n",
    "no_of_batches = int(len(X_train) / batch_size)\n",
    "\n",
    "data = tf.placeholder(tf.float32, [None, max_sequence_size, row_size])\n",
    "target = tf.placeholder(tf.float32, [None, num_classes])\n",
    "model = VariableSequenceClassification(data, target)\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch in range(4):\n",
    "    ptr = 0\n",
    "    for _ in range(no_of_batches):\n",
    "        inp, out = X_train[ptr:ptr+batch_size], y_train[ptr:ptr+batch_size]\n",
    "        ptr += batch_size\n",
    "        sess.run(model.optimize, {data: inp, target: out})\n",
    "    error = sess.run(model.error, {data: X_test, target: y_test})\n",
    "    print('Epoch {:2d} error {:3.1f}%'.format(epoch + 1, 100 * error))\n",
    "    print(sess.run(model.prediction,{data: X_test}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
