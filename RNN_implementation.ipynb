{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASSISTments Data Mining Competition 2017 - Optional Semester Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import functools\n",
    "import tensorflow as tf\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Constants\n",
    "DATA_DIR = 'Data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load all the data into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_train_logs = pd.read_pickle('student_train_logs')\n",
    "student_test_logs = pd.read_pickle('student_test_logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = student_train_logs['ITEST_id'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the training labels into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = pd.read_csv('Data/training_label.csv', index_col='ITEST_id').sort_index()\n",
    "train_labels.drop_duplicates(subset=None, keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEM examples:  117\n",
      "Non STEM examples: 117\n"
     ]
    }
   ],
   "source": [
    "STEM_idx = train_labels[train_labels['isSTEM'] == 1].index.unique()\n",
    "non_STEM_idx = train_labels[train_labels['isSTEM'] == 0].index.unique()[:len(STEM_idx)]\n",
    "\n",
    "print('STEM examples: ', len(STEM_idx))\n",
    "print('Non STEM examples:', len(non_STEM_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use an RNN (LSTM), we need to create sequences of actions. A sequence is a matrix in which each row is an action taken by a student that follows a chronological order. An action is a row in the provided dataset (see above). In summary, we produce a Tensor (a 3D array) with the following dimensions:\n",
    "\n",
    "* First: All the students actions\n",
    "* Second: A list of actions for a specific student\n",
    "* Third: A specific action (which in turn is a vector of metrics (see above))\n",
    "\n",
    "The output that interests us is the last output of the sequence. Which is the output consisting of the last action of a specific student taking into account all previous actions of that student. This output will represent if the student will choose a STEM career or not.\n",
    "\n",
    "**A problem that we will encounter and will need to solve is that sequences have dynamic lengths**. Because Tensors need to be of fixed predefined size. Meaning that not all students have done the same number of actions while learning.\n",
    "\n",
    "A possible solution could be to use *padding*. Padding means normalizing the size of all sequences (to the maximum of all sequences size) and add 0 vectors for sequences that are smaller than the maximum size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of the input tensor\n",
    "train_input = []\n",
    "\n",
    "for idx in STEM_idx:\n",
    "    specific_student_actions = student_train_logs[student_train_logs['ITEST_id'] == idx]\n",
    "    student_actions = []\n",
    "    \n",
    "    for action_idx, action in specific_student_actions.iterrows():\n",
    "        student_actions.append(np.array(action))\n",
    "    \n",
    "    train_input.append(np.array(student_actions))\n",
    "\n",
    "for idx in non_STEM_idx:\n",
    "    specific_student_actions = student_train_logs[student_train_logs['ITEST_id'] == idx]\n",
    "    student_actions = []\n",
    "    \n",
    "    for action_idx, action in specific_student_actions.iterrows():\n",
    "        student_actions.append(np.array(action))\n",
    "    \n",
    "    train_input.append(np.array(student_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "234"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, first student has taken 504 actions while the second student has taken 129 actions.\n",
    "\n",
    "We retrieve the biggest sequence and increase the size of all other sequences to be equal to the maximum with 0 vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1863"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sequence_size = 0\n",
    "for i in range(len(train_input)):\n",
    "    max_sequence_size = max(max_sequence_size, len(train_input[i]))\n",
    "max_sequence_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_padd = []\n",
    "n_features = len(train_input[0][0])\n",
    "\n",
    "for i in range(len(train_input)):\n",
    "    n = len(train_input[i])\n",
    "    rem = max_sequence_size - n\n",
    "\n",
    "    z = np.zeros((rem, n_features), dtype=train_input[i].dtype)\n",
    "    train_input_padd.append(np.concatenate((train_input[i], z)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1863, 11)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input_padd[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "shuffle(train_input_padd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train output data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training output is a list of one-hot vectors of size 2 (one for each student). If the value of index 0 is 1, then it is non-STEM, if value at index 1 is 1, then it is STEM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_output = []\n",
    "\n",
    "for seq in train_input_padd:\n",
    "    idx = int(seq[0][0])\n",
    "    temp_arr = ([0] * 2)\n",
    "    \n",
    "    if train_labels.loc[idx]['isSTEM'] == 1:\n",
    "        temp_arr[1] = 1\n",
    "    else:\n",
    "        temp_arr[0] = 1\n",
    "\n",
    "    train_output.append(temp_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([117, 117])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(train_output, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will split the training data into two sets, one for training and the other for testing. We will randomly take 10% of the data for testing and use the rest as training data. Ultimately we will train with everything before predicting.\n",
    "\n",
    "**TODO** Améliorer ça et utiliser kFold cross validation (stratified), parce qu'on a relativement peu de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "NUM_TEST = int(np.ceil(len(train_input_padd) * 0.1))\n",
    "print(NUM_TEST)\n",
    "X_train = train_input_padd[NUM_TEST:]\n",
    "y_train = train_output[NUM_TEST:]\n",
    "X_test = train_input_padd[:NUM_TEST]\n",
    "y_test = train_output[:NUM_TEST]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of students for training: 210\n",
      "Number of students for testing: 24\n"
     ]
    }
   ],
   "source": [
    "print('Number of students for training:', len(X_train))\n",
    "print('Number of students for testing:', len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we fist create placeholders to hold our data. The dimensions for data are [Batch Size, Sequence Length, Input Dimension]. Here our values are:\n",
    "\n",
    "* Batch size: to be defined at runtime\n",
    "* Sequence length: Different for each student\n",
    "* Input Dimension: Known (number of features of the data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lazy_property(function):\n",
    "    attribute = '_' + function.__name__\n",
    "\n",
    "    @property\n",
    "    @functools.wraps(function)\n",
    "    def wrapper(self):\n",
    "        if not hasattr(self, attribute):\n",
    "            setattr(self, attribute, function(self))\n",
    "        return getattr(self, attribute)\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "class VariableSequenceClassification:\n",
    "\n",
    "    def __init__(self, data, target, num_hidden=50, num_layers=2):\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self._num_hidden = num_hidden\n",
    "        self._num_layers = num_layers\n",
    "        self.prediction\n",
    "        self.error\n",
    "        self.optimize\n",
    "\n",
    "    @lazy_property\n",
    "    def length(self):\n",
    "        used = tf.sign(tf.reduce_max(tf.abs(self.data), reduction_indices=2))\n",
    "        length = tf.reduce_sum(used, reduction_indices=1)\n",
    "        length = tf.cast(length, tf.int32)\n",
    "        return length\n",
    "\n",
    "    @lazy_property\n",
    "    def prediction(self):\n",
    "        # Recurrent network.\n",
    "        output, _ = tf.nn.dynamic_rnn(\n",
    "            tf.nn.rnn_cell.LSTMCell(self._num_hidden),\n",
    "            data,\n",
    "            dtype=tf.float32,\n",
    "            sequence_length=self.length,\n",
    "        )\n",
    "        last = self._last_relevant(output, self.length)\n",
    "        # Softmax layer.\n",
    "        weight, bias = self._weight_and_bias(\n",
    "            self._num_hidden, int(self.target.get_shape()[1]))\n",
    "        prediction = tf.nn.softmax(tf.matmul(last, weight) + bias)\n",
    "        return prediction\n",
    "\n",
    "    @lazy_property\n",
    "    def cost(self):\n",
    "        cross_entropy = -tf.reduce_sum(self.target * tf.log(self.prediction))\n",
    "        return cross_entropy\n",
    "\n",
    "    @lazy_property\n",
    "    def optimize(self):\n",
    "        learning_rate = 0.03\n",
    "        optimizer = tf.train.RMSPropOptimizer(learning_rate)\n",
    "        return optimizer.minimize(self.cost)\n",
    "\n",
    "    @lazy_property\n",
    "    def error(self):\n",
    "        mistakes = tf.not_equal(\n",
    "            tf.argmax(self.target, 1), tf.argmax(self.prediction, 1))\n",
    "        return tf.reduce_mean(tf.cast(mistakes, tf.float32))\n",
    "\n",
    "    @staticmethod\n",
    "    def _weight_and_bias(in_size, out_size):\n",
    "        weight = tf.truncated_normal([in_size, out_size], stddev=0.01)\n",
    "        bias = tf.constant(0.1, shape=[out_size])\n",
    "        return tf.Variable(weight), tf.Variable(bias)\n",
    "\n",
    "    @staticmethod\n",
    "    def _last_relevant(output, length):\n",
    "        batch_size = tf.shape(output)[0]\n",
    "        max_length = int(output.get_shape()[1])\n",
    "        output_size = int(output.get_shape()[2])\n",
    "        index = tf.range(0, batch_size) * max_length + (length - 1)\n",
    "        flat = tf.reshape(output, [-1, output_size])\n",
    "        relevant = tf.gather(flat, index)\n",
    "        return relevant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flow execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We execute the flow execution graph defined before with our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [1, 0],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [0, 1]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dario/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:95: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 error test 62.5%\n",
      "Epoch  1 error train 48.6%\n",
      "[[ 0.63236457  0.36763549]\n",
      " [ 0.84048969  0.15951037]\n",
      " [ 0.85096335  0.14903665]\n",
      " [ 0.84088129  0.15911879]\n",
      " [ 0.62844461  0.37155542]\n",
      " [ 0.82852036  0.1714796 ]\n",
      " [ 0.62844461  0.37155542]\n",
      " [ 0.83954233  0.16045761]\n",
      " [ 0.68002713  0.31997287]\n",
      " [ 0.62844461  0.37155542]\n",
      " [ 0.62844521  0.37155479]\n",
      " [ 0.8412739   0.15872611]\n",
      " [ 0.62846595  0.37153396]\n",
      " [ 0.62844461  0.37155542]\n",
      " [ 0.62844461  0.37155542]\n",
      " [ 0.62844461  0.37155542]\n",
      " [ 0.62844461  0.37155542]\n",
      " [ 0.62828845  0.37171152]\n",
      " [ 0.68344408  0.31655586]\n",
      " [ 0.62960023  0.37039977]\n",
      " [ 0.78131527  0.21868473]\n",
      " [ 0.68002713  0.31997287]\n",
      " [ 0.76301497  0.23698504]\n",
      " [ 0.62844467  0.37155536]]\n",
      "Epoch  2 error test 62.5%\n",
      "Epoch  2 error train 48.6%\n",
      "[[ 0.508825    0.491175  ]\n",
      " [ 0.8330673   0.16693266]\n",
      " [ 0.86583841  0.13416162]\n",
      " [ 0.8330673   0.16693266]\n",
      " [ 0.508825    0.491175  ]\n",
      " [ 0.86848634  0.13151368]\n",
      " [ 0.508825    0.491175  ]\n",
      " [ 0.8330673   0.16693266]\n",
      " [ 0.508825    0.491175  ]\n",
      " [ 0.508825    0.491175  ]\n",
      " [ 0.508825    0.491175  ]\n",
      " [ 0.8330673   0.16693266]\n",
      " [ 0.508825    0.491175  ]\n",
      " [ 0.50894535  0.49105462]\n",
      " [ 0.508825    0.491175  ]\n",
      " [ 0.508825    0.491175  ]\n",
      " [ 0.508825    0.491175  ]\n",
      " [ 0.508825    0.491175  ]\n",
      " [ 0.508825    0.491175  ]\n",
      " [ 0.508825    0.491175  ]\n",
      " [ 0.86720264  0.13279738]\n",
      " [ 0.508825    0.491175  ]\n",
      " [ 0.8330673   0.16693266]\n",
      " [ 0.508825    0.491175  ]]\n",
      "Epoch  3 error test 25.0%\n",
      "Epoch  3 error train 28.6%\n",
      "[[ 0.47802022  0.52197981]\n",
      " [ 0.88030833  0.11969159]\n",
      " [ 0.89170986  0.10829017]\n",
      " [ 0.88874668  0.11125333]\n",
      " [ 0.47802022  0.52197981]\n",
      " [ 0.90450537  0.09549468]\n",
      " [ 0.58895022  0.41104981]\n",
      " [ 0.88030833  0.11969159]\n",
      " [ 0.47802022  0.52197981]\n",
      " [ 0.47802022  0.52197981]\n",
      " [ 0.47802109  0.52197891]\n",
      " [ 0.88030833  0.11969159]\n",
      " [ 0.47802457  0.52197546]\n",
      " [ 0.58041704  0.41958302]\n",
      " [ 0.47804275  0.52195722]\n",
      " [ 0.58042526  0.41957474]\n",
      " [ 0.47806227  0.52193779]\n",
      " [ 0.47802022  0.52197981]\n",
      " [ 0.47802022  0.52197981]\n",
      " [ 0.47802031  0.52197975]\n",
      " [ 0.90870857  0.09129144]\n",
      " [ 0.47802022  0.52197981]\n",
      " [ 0.88030833  0.11969159]\n",
      " [ 0.47802022  0.52197981]]\n",
      "Epoch  4 error test 54.2%\n",
      "Epoch  4 error train 40.0%\n",
      "[[ 0.50460267  0.49539736]\n",
      " [ 0.82567269  0.17432736]\n",
      " [ 0.87846684  0.12153315]\n",
      " [ 0.88158286  0.11841718]\n",
      " [ 0.50460267  0.49539736]\n",
      " [ 0.88066387  0.11933614]\n",
      " [ 0.50460267  0.49539736]\n",
      " [ 0.82567269  0.17432736]\n",
      " [ 0.38767436  0.61232567]\n",
      " [ 0.50460267  0.49539736]\n",
      " [ 0.50460267  0.49539736]\n",
      " [ 0.84047097  0.159529  ]\n",
      " [ 0.50460267  0.49539736]\n",
      " [ 0.50460267  0.49539736]\n",
      " [ 0.50460267  0.49539736]\n",
      " [ 0.50460267  0.49539736]\n",
      " [ 0.50460267  0.49539736]\n",
      " [ 0.50460267  0.49539736]\n",
      " [ 0.50460267  0.49539736]\n",
      " [ 0.50460261  0.49539733]\n",
      " [ 0.8917706   0.10822935]\n",
      " [ 0.38767436  0.61232567]\n",
      " [ 0.82567269  0.17432736]\n",
      " [ 0.50460267  0.49539736]]\n",
      "Epoch  5 error test 62.5%\n",
      "Epoch  5 error train 48.6%\n",
      "[[ 0.50995106  0.49004894]\n",
      " [ 0.83860832  0.16139172]\n",
      " [ 0.84162843  0.15837161]\n",
      " [ 0.83860832  0.16139172]\n",
      " [ 0.50995106  0.49004894]\n",
      " [ 0.87859672  0.12140324]\n",
      " [ 0.50995106  0.49004894]\n",
      " [ 0.83860832  0.16139172]\n",
      " [ 0.50995106  0.49004894]\n",
      " [ 0.50995106  0.49004894]\n",
      " [ 0.50995106  0.49004894]\n",
      " [ 0.83860832  0.16139172]\n",
      " [ 0.50995106  0.49004894]\n",
      " [ 0.59638256  0.40361744]\n",
      " [ 0.50995106  0.49004894]\n",
      " [ 0.57346743  0.4265326 ]\n",
      " [ 0.50995106  0.49004894]\n",
      " [ 0.50995106  0.49004894]\n",
      " [ 0.50995106  0.49004894]\n",
      " [ 0.50995106  0.490049  ]\n",
      " [ 0.89138228  0.10861772]\n",
      " [ 0.50995106  0.49004894]\n",
      " [ 0.84437388  0.15562615]\n",
      " [ 0.50995106  0.49004894]]\n",
      "Epoch  6 error test 62.5%\n",
      "Epoch  6 error train 48.6%\n",
      "[[ 0.51724666  0.48275328]\n",
      " [ 0.83274555  0.16725448]\n",
      " [ 0.83274555  0.16725448]\n",
      " [ 0.83274555  0.16725448]\n",
      " [ 0.52556348  0.47443652]\n",
      " [ 0.87927419  0.12072588]\n",
      " [ 0.52556348  0.47443652]\n",
      " [ 0.83274555  0.16725448]\n",
      " [ 0.51724666  0.48275334]\n",
      " [ 0.52556348  0.47443652]\n",
      " [ 0.52556348  0.47443652]\n",
      " [ 0.83274555  0.16725448]\n",
      " [ 0.52556348  0.47443652]\n",
      " [ 0.52556348  0.47443652]\n",
      " [ 0.52556348  0.47443652]\n",
      " [ 0.52556348  0.47443652]\n",
      " [ 0.52556348  0.47443652]\n",
      " [ 0.52556348  0.47443652]\n",
      " [ 0.51724666  0.48275334]\n",
      " [ 0.52556348  0.47443655]\n",
      " [ 0.89695573  0.10304423]\n",
      " [ 0.51724666  0.48275334]\n",
      " [ 0.82702392  0.17297611]\n",
      " [ 0.52556348  0.47443652]]\n",
      "Epoch  7 error test 62.5%\n",
      "Epoch  7 error train 48.6%\n",
      "[[ 0.52961099  0.47038901]\n",
      " [ 0.8398782   0.1601218 ]\n",
      " [ 0.8398782   0.1601218 ]\n",
      " [ 0.8398782   0.1601218 ]\n",
      " [ 0.52961099  0.47038901]\n",
      " [ 0.92127967  0.07872029]\n",
      " [ 0.52961099  0.47038901]\n",
      " [ 0.8398782   0.1601218 ]\n",
      " [ 0.52961099  0.47038901]\n",
      " [ 0.52961099  0.47038901]\n",
      " [ 0.52961099  0.47038901]\n",
      " [ 0.8398782   0.1601218 ]\n",
      " [ 0.52961099  0.47038901]\n",
      " [ 0.52961099  0.47038901]\n",
      " [ 0.52961099  0.47038901]\n",
      " [ 0.52961099  0.47038901]\n",
      " [ 0.52961099  0.47038901]\n",
      " [ 0.52961099  0.47038901]\n",
      " [ 0.52961099  0.47038901]\n",
      " [ 0.52961099  0.47038904]\n",
      " [ 0.92048579  0.07951421]\n",
      " [ 0.52961099  0.47038901]\n",
      " [ 0.83632416  0.16367583]\n",
      " [ 0.52961099  0.47038901]]\n",
      "Epoch  8 error test 62.5%\n",
      "Epoch  8 error train 48.6%\n",
      "[[ 0.52110744  0.47889262]\n",
      " [ 0.83397448  0.1660255 ]\n",
      " [ 0.83397448  0.1660255 ]\n",
      " [ 0.83397448  0.1660255 ]\n",
      " [ 0.52110744  0.47889262]\n",
      " [ 0.92442811  0.07557188]\n",
      " [ 0.52110744  0.47889262]\n",
      " [ 0.83397448  0.1660255 ]\n",
      " [ 0.52110744  0.47889262]\n",
      " [ 0.52110744  0.47889262]\n",
      " [ 0.52110744  0.47889262]\n",
      " [ 0.83397448  0.1660255 ]\n",
      " [ 0.52110744  0.47889262]\n",
      " [ 0.52110744  0.47889262]\n",
      " [ 0.52110744  0.47889262]\n",
      " [ 0.52110744  0.47889262]\n",
      " [ 0.52110744  0.47889262]\n",
      " [ 0.52110744  0.47889262]\n",
      " [ 0.52110744  0.47889262]\n",
      " [ 0.52110744  0.47889262]\n",
      " [ 0.92442775  0.07557223]\n",
      " [ 0.52110744  0.47889262]\n",
      " [ 0.8363409   0.16365913]\n",
      " [ 0.52110744  0.47889262]]\n",
      "Epoch  9 error test 62.5%\n",
      "Epoch  9 error train 48.6%\n",
      "[[ 0.51300627  0.4869937 ]\n",
      " [ 0.8383531   0.16164693]\n",
      " [ 0.8383531   0.16164693]\n",
      " [ 0.8383531   0.16164693]\n",
      " [ 0.51300627  0.4869937 ]\n",
      " [ 0.93194652  0.06805351]\n",
      " [ 0.51300627  0.4869937 ]\n",
      " [ 0.8383531   0.16164693]\n",
      " [ 0.51300627  0.4869937 ]\n",
      " [ 0.51300627  0.4869937 ]\n",
      " [ 0.51300627  0.4869937 ]\n",
      " [ 0.8383531   0.16164693]\n",
      " [ 0.51300627  0.4869937 ]\n",
      " [ 0.51300627  0.4869937 ]\n",
      " [ 0.51300627  0.4869937 ]\n",
      " [ 0.51300627  0.4869937 ]\n",
      " [ 0.51300627  0.4869937 ]\n",
      " [ 0.51300627  0.4869937 ]\n",
      " [ 0.51300627  0.4869937 ]\n",
      " [ 0.51300627  0.4869937 ]\n",
      " [ 0.93194902  0.06805104]\n",
      " [ 0.51300627  0.4869937 ]\n",
      " [ 0.84650749  0.15349251]\n",
      " [ 0.51300627  0.4869937 ]]\n",
      "Epoch 10 error test 62.5%\n",
      "Epoch 10 error train 48.6%\n",
      "[[ 0.51071471  0.48928523]\n",
      " [ 0.82510054  0.17489947]\n",
      " [ 0.82510054  0.17489947]\n",
      " [ 0.82510054  0.17489947]\n",
      " [ 0.51071471  0.48928523]\n",
      " [ 0.929286    0.07071405]\n",
      " [ 0.51071471  0.48928523]\n",
      " [ 0.82510054  0.17489947]\n",
      " [ 0.51071471  0.48928523]\n",
      " [ 0.51071471  0.48928523]\n",
      " [ 0.51071471  0.48928523]\n",
      " [ 0.82510054  0.17489947]\n",
      " [ 0.51071471  0.48928523]\n",
      " [ 0.51071471  0.48928523]\n",
      " [ 0.51071471  0.48928523]\n",
      " [ 0.51071471  0.48928523]\n",
      " [ 0.51071471  0.48928523]\n",
      " [ 0.51071471  0.48928523]\n",
      " [ 0.51071471  0.48928523]\n",
      " [ 0.51071471  0.48928523]\n",
      " [ 0.92930204  0.07069797]\n",
      " [ 0.51071471  0.48928523]\n",
      " [ 0.84455472  0.15544534]\n",
      " [ 0.51071471  0.48928523]]\n"
     ]
    }
   ],
   "source": [
    "num_classes = 2\n",
    "row_size = n_features\n",
    "batch_size = 10\n",
    "no_of_batches = int(len(X_train) / batch_size)\n",
    "\n",
    "data = tf.placeholder(tf.float32, [None, max_sequence_size, row_size])\n",
    "target = tf.placeholder(tf.float32, [None, num_classes])\n",
    "model = VariableSequenceClassification(data, target)\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch in range(10):\n",
    "    ptr = 0\n",
    "    for _ in range(no_of_batches):\n",
    "        inp, out = X_train[ptr:ptr+batch_size], y_train[ptr:ptr+batch_size]\n",
    "        ptr += batch_size\n",
    "        sess.run(model.optimize, {data: inp, target: out})\n",
    "    error_test = sess.run(model.error, {data: X_test, target: y_test})\n",
    "    error_train = sess.run(model.error, {data: X_train, target: y_train})\n",
    "    print('Epoch {:2d} error test {:3.1f}%'.format(epoch + 1, 100 * error_test))\n",
    "    print('Epoch {:2d} error train {:3.1f}%'.format(epoch + 1, 100 * error_train))\n",
    "    print(sess.run(model.prediction,{data: X_test}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
