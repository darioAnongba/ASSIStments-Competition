{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "DATA_DIR = 'Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_test_logs = pd.read_pickle(DATA_DIR + 'student_test_logs')\n",
    "student_train_logs = pd.read_pickle(DATA_DIR + 'student_train_logs')\n",
    "print(student_train_logs.shape)\n",
    "student_train_logs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_labels = pd.read_csv('Data/training_label.csv', index_col='ITEST_id').sort_index()\n",
    "train_labels.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "train_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, fixed_dim, output_dim, batch_size=1, n_layers=1):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=input_dim,\n",
    "                            hidden_size=hidden_dim,\n",
    "                            num_layers=n_layers,\n",
    "                            dropout=0.25,\n",
    "                            bidirectional=True)\n",
    "        self.hidden2label = nn.Linear(hidden_dim*2 + fixed_dim, output_dim)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        h0 = Variable(torch.zeros(self.n_layers*2, self.batch_size, self.hidden_dim))\n",
    "        c0 = Variable(torch.zeros(self.n_layers*2, self.batch_size, self.hidden_dim))\n",
    "        return (h0, c0)\n",
    "\n",
    "    def forward(self, actions, fixed):\n",
    "        hidden_state = self.init_hidden()\n",
    "        out, _ = self.lstm(actions, hidden_state)\n",
    "        y = self.hidden2label(torch.cat([out[-1, :, :], fixed], dim=1))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingSet(Dataset):\n",
    "    def __init__(self):\n",
    "        self.actions = student_train_logs.drop(['SY ASSISTments Usage',\n",
    "                                        'AveKnow',\n",
    "                                        'AveCarelessness',\n",
    "                                        'AveCorrect',\n",
    "                                        'NumActions',\n",
    "                                        'AveResBored',\n",
    "                                        'AveResEngcon',\n",
    "                                        'AveResConf',\n",
    "                                        'AveResFrust',\n",
    "                                        'AveResOfftask',\n",
    "                                        'AveResGaming'], axis=1)\n",
    "        \n",
    "        self.fixed = student_train_logs[['ITEST_id',\n",
    "                                        'SY ASSISTments Usage',\n",
    "                                        'AveKnow',\n",
    "                                        'AveCarelessness',\n",
    "                                        'AveCorrect',\n",
    "                                        'NumActions',\n",
    "                                        'AveResBored',\n",
    "                                        'AveResEngcon',\n",
    "                                        'AveResConf',\n",
    "                                        'AveResFrust',\n",
    "                                        'AveResOfftask',\n",
    "                                        'AveResGaming']]\n",
    "                \n",
    "        self.y = train_labels\n",
    "\n",
    "        self.weights = [0.7, 0.3]\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, id):\n",
    "        studentId = self.y.iloc[id].name\n",
    "        actions = self.actions[self.actions['ITEST_id'] == studentId].as_matrix().astype(np.float32)\n",
    "        \n",
    "        fixed_df = self.fixed[self.fixed['ITEST_id'] == studentId].assign(MCAS=self.y.loc[studentId].MCAS, SchoolId=self.y.loc[studentId].SchoolId)\n",
    "        fixed = fixed_df.as_matrix().astype(np.float32)[0]\n",
    "        \n",
    "        target = np.asarray([self.y.loc[studentId].isSTEM]).astype(np.float32)\n",
    "        \n",
    "        return torch.from_numpy(actions), torch.from_numpy(fixed), torch.from_numpy(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_dataset = TrainingSet()\n",
    "sampler = WeightedRandomSampler(train_dataset.weights, num_samples=len(train_dataset))\n",
    "train_loader = DataLoader(train_dataset, sampler=sampler, batch_size=1, num_workers=4)\n",
    "\n",
    "n_hidden = 32\n",
    "lstm = LSTMClassifier(train_dataset.actions.shape[1] - 1,\n",
    "                      n_hidden,\n",
    "                      train_dataset.fixed.shape[1] - 1 + 2, # + 2 for MCAS and SchoolId\n",
    "                      output_dim=1,\n",
    "                      batch_size=1,\n",
    "                      n_layers=1)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "learning_rate = 0.001 # If you set this too high, it might explode. If too low, it might not learn\n",
    "optimizer = optim.Adam(lstm.parameters(), lr=learning_rate)\n",
    "\n",
    "losses = []\n",
    "accs = []\n",
    "bar1 = tqdm_notebook(range(5))\n",
    "for epoch in bar1:\n",
    "    train_loss = 0\n",
    "    train_acc = []\n",
    "    \n",
    "    for i, (actions, fixed, target) in enumerate(tqdm_notebook(train_loader)):\n",
    "        actions = Variable(actions[:, :, 1:]).permute(1,0,2)\n",
    "        fixed = Variable(fixed[:, 1:])\n",
    "        target = Variable(target)\n",
    "\n",
    "        lstm.zero_grad()\n",
    "        output = lstm(actions, fixed)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.data[0]\n",
    "        \n",
    "        _, argmax = output.data.max(1)\n",
    "        y_preds = argmax.view(-1).numpy()\n",
    "        y_true = target.data.view(-1).numpy()\n",
    "\n",
    "        train_acc.append(accuracy_score(y_true, y_preds))\n",
    "\n",
    "    train_loss /= i+1\n",
    "    \n",
    "    losses.append(train_loss)\n",
    "    accs.append(np.mean(train_acc))\n",
    "    \n",
    "    bar1.set_postfix(loss=losses[-1], acc=accs[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.plot(accs)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
