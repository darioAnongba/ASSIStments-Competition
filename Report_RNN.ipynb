{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASSISTments Data Mining Competition 2017 - Report of RNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import WeightedRandomSampler, Sampler\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import random\n",
    "\n",
    "DATA_DIR = 'Data/'\n",
    "RESULTS_DIR = 'Results/'\n",
    "SEED = 7\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "categorical_features = set(['skill',\n",
    "                        'problemId',\n",
    "                        'assignmentId',\n",
    "                        'assistmentId',\n",
    "                        'problemType'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the training data\n",
    "\n",
    "The training data is stored in pickles as a dictionary where the keys are student ids and and values are tuples of **(Sequence of dynamic features, array of fixed features, target)**:\n",
    "\n",
    "- **Dynamic features**: Features that can potentially contain different values per action in a sequence. Stored as a pandas dataframe sorted by time.\n",
    "- **Fixed features**: Features that are unchanged through time. Ex: Average correctness, the school id or MCAS grade. Stored as a pandas Series.\n",
    "- **target**: Either yes (1) or no (0) the student has chosen a carrer in STEM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_train = open(DATA_DIR + \"student_train_logs.pickle\",\"rb\")\n",
    "train = pickle.load(pickle_train)\n",
    "\n",
    "train[9][0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "Here we define the parameters of the whole system, each parameter can be tweaked as needed:\n",
    "\n",
    "** System parameters**\n",
    "- **Validation_set_size** (int): The size of the validation set. Due to the poor amount of data that we are provided, the size of the validation set should be small.\n",
    "- **use_gpu** (bool): Use CUDA or not\n",
    "- **num_workers** (int): Maximum number of threads on the CPU.\n",
    "- **epochs** (int): Number of epochs\n",
    "\n",
    "** Model (RNN) parameters**\n",
    "- **dynamic_dim** (int): Number of dynamic features (that change at every action)\n",
    "- **fixed_dim** (int): Number of the static features (that are unchanged at every action)\n",
    "- **hidden_dim** (int): Number of features to be output by the Recurrent neural network \n",
    "- **n_layers** (int): Number of layers of the RNN\n",
    "- **bidirectional** (bool): To either use a bidirectional RNN or not\n",
    "- **dropout** (float in [0, 1]): Dropout of the RNN\n",
    "- **learning_rate** (float in [0, 1]): Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'validation_set_size': 30,\n",
    "    'dynamic_dim': train[9][0].shape[1],\n",
    "    'fixed_dim': train[9][1].shape[0],\n",
    "    'hidden_dim': 256,\n",
    "    'dropout': 0.25,\n",
    "    'n_layers': 3, \n",
    "    'bidirectional': True,\n",
    "    'use_gpu': True,\n",
    "    'learning_rate': 1e-3,\n",
    "    'epochs': 20,\n",
    "    'num_workers': 4\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the validation set\n",
    "\n",
    "We create the validation set by randomly selecting `validation_set_size` students from the total training set and discarding those students from the actual training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = {k:v for k, v in random.sample(train.items(), parameters['validation_set_size'])}\n",
    "train_truncated = { k : train[k] for k in set(train) - set(validation) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Data Loader\n",
    "\n",
    "Now, we implement a data loader that will enable us to iterate through our data and transform our data into tensors to be used with pyTorch.\n",
    "\n",
    "With PyTorch, every DataLoader can be set a sampler that will define how the data is being sampled.\n",
    "Here we do not define any sampler but it could be a good idea to create a random weighted sampler in order to balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataSet(Dataset):\n",
    "    def __init__(self, sequences):\n",
    "        self.idx = list(sequences.keys())\n",
    "        self.sequences = sequences\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return 2\n",
    "        #return len(self.sequences)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, id):\n",
    "        student_id = self.idx[id]\n",
    "        \n",
    "        dynamic = self.sequences[student_id][0].as_matrix().astype(np.float32)\n",
    "        fixed = self.sequences[student_id][1].as_matrix().astype(np.float32)\n",
    "        target = np.asarray([self.sequences[student_id][2]]).astype(np.float32)\n",
    "\n",
    "        return student_id, dynamic, fixed, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DataSet(train_truncated)\n",
    "validation_dataset = DataSet(validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the RNN model\n",
    "\n",
    "Our RNN is composed of an LSTM or GRU that takes as data our sequence of actions per student. The LSTM outputs a number of hidden parameters on which we append our fixed features. We then fave a fully connected layer that takes as input our concatenated features and outputs a value. Finally, we use the sigmoid function to output a probability that the student will effectively do a career in STEM.\n",
    "\n",
    "Here are the functions explained:\n",
    "- **init**: Initializes the model by setting the parameters, creating a RNN layer and a Linear layer (fully connected).\n",
    "- **init_hidden**: Creates a zeroed initial hidden state.\n",
    "- **forward**: Actual RNN computation.\n",
    "- **step**: A step in the training process.\n",
    "- **evaluate_val**: Validation set results on the current model.\n",
    "- **fit**: Complete training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 hidden_dim,\n",
    "                 fixed_dim,\n",
    "                 n_layers,\n",
    "                 bi,\n",
    "                 use_gpu,\n",
    "                 dropout,\n",
    "                 lr,\n",
    "                 output_dim=1,\n",
    "                 batch_size=1):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.bi = bi\n",
    "        self.output_dim = output_dim\n",
    "        self.use_gpu = use_gpu\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.gru = nn.GRU(input_size=input_dim,\n",
    "                          hidden_size=hidden_dim,\n",
    "                          num_layers=n_layers,\n",
    "                          dropout=dropout,\n",
    "                          bidirectional=bi)\n",
    "        if bi:\n",
    "            self.decoder = nn.Linear(hidden_dim*2 + fixed_dim, output_dim)\n",
    "        else:\n",
    "            self.decoder = nn.Linear(hidden_dim + fixed_dim, output_dim)\n",
    "        \n",
    "\n",
    "    def init_hidden(self):\n",
    "        if self.bi:\n",
    "            if self.use_gpu:\n",
    "                return Variable(torch.zeros(self.n_layers*2, self.batch_size, self.hidden_dim)).cuda()\n",
    "            else:\n",
    "                return Variable(torch.zeros(self.n_layers*2, self.batch_size, self.hidden_dim))\n",
    "        \n",
    "        if self.use_gpu:\n",
    "            return Variable(torch.zeros(self.n_layers, self.batch_size, self.hidden_dim)).cuda()\n",
    "        else:\n",
    "            return Variable(torch.zeros(self.n_layers, self.batch_size, self.hidden_dim))\n",
    "\n",
    "    \n",
    "    def forward(self, actions, fixed):\n",
    "        hidden_state = self.init_hidden()\n",
    "        out, _ = self.gru(actions, hidden_state)\n",
    "        out = self.decoder(torch.cat([out[-1, :, :], fixed], dim=1))                                                                                                   \n",
    "        out = out.view(self.batch_size, self.output_dim)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    \n",
    "    def step(self, dynamic, fixed, target):                                                                                                        \n",
    "        self.zero_grad()                                                                                                           \n",
    "        output = self.forward(dynamic, fixed)                                                                                                      \n",
    "        loss = self.criterion(output, target.float())                                                                      \n",
    "        loss.backward()                                                                                                                 \n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.data[0], F.sigmoid(output)\n",
    "    \n",
    "    \n",
    "    def evaluate_val(self, dataset):\n",
    "        loader = DataLoader(dataset, self.batch_size, num_workers=parameters['num_workers'])\n",
    "        \n",
    "        y_preds = []\n",
    "        y_true = []\n",
    "        \n",
    "        for i, (_, actions, fixed, target) in enumerate(tqdm_notebook(loader, leave=False)):\n",
    "            y_true.append(target.float())\n",
    "            \n",
    "            actions = actions.permute(1, 0, 2)\n",
    "            \n",
    "            if self.use_gpu:\n",
    "                actions = Variable(actions).cuda()\n",
    "                fixed = Variable(fixed).cuda()\n",
    "            else:\n",
    "                actions = Variable(actions)\n",
    "                fixed = Variable(fixed)\n",
    "                \n",
    "            output = self.forward(actions, fixed)\n",
    "            output = F.sigmoid(output)\n",
    "            \n",
    "            if self.use_gpu:\n",
    "                y_preds.append(output.squeeze().cpu().data[0])\n",
    "            else:\n",
    "                y_preds.append(output.squeeze().data[0])\n",
    "                \n",
    "        return y_true, y_preds\n",
    "    \n",
    "    \n",
    "    def fit(self, train_dataset):\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "        self.optimizer = optim.Adamax(self.parameters(), lr=parameters['learning_rate'])\n",
    "        \n",
    "        loader = DataLoader(train_dataset, batch_size=self.batch_size, num_workers=parameters['num_workers'], shuffle=True)\n",
    "        \n",
    "        e_losses = []\n",
    "        e_accs = []\n",
    "        e_aucs = []\n",
    "        \n",
    "        e_val_accs = []\n",
    "        e_val_aucs = []\n",
    "        \n",
    "        e_bar = tqdm_notebook(range(parameters['epochs']))\n",
    "        for e in e_bar:\n",
    "            self.train()\n",
    "            e_loss = 0\n",
    "            preds = []\n",
    "            targets = []\n",
    "            val_preds = []\n",
    "            \n",
    "            for i, (_, seq, fixed, label) in enumerate(tqdm_notebook(loader, leave=False)):\n",
    "                seq = seq.permute(1,0,2)\n",
    "                \n",
    "                if self.use_gpu:\n",
    "                    seq_var = Variable(seq).cuda()\n",
    "                    fixed_var = Variable(fixed).cuda()\n",
    "                    label_var = Variable(label).cuda()\n",
    "                else:\n",
    "                    seq_var = Variable(seq)\n",
    "                    fixed_var = Variable(fixed)\n",
    "                    label_var =  Variable(label)\n",
    "                \n",
    "                loss, output = self.step(seq_var, fixed_var, label_var)\n",
    "                e_loss += loss\n",
    "                \n",
    "                preds.append(output.squeeze().cpu().data[0])\n",
    "                targets.append(label.float())\n",
    "            \n",
    "            preds = np.array(preds)\n",
    "            targets = np.array(targets)\n",
    "            auc = roc_auc_score(targets, preds)\n",
    "            \n",
    "            preds[preds >= 0.5] = 1\n",
    "            preds[preds < 0.5] = 0\n",
    "            acc = accuracy_score(preds, targets)\n",
    "            \n",
    "            e_losses.append(e_loss / (i+1))\n",
    "            e_accs.append(acc)\n",
    "            e_aucs.append(auc)\n",
    "\n",
    "            # Validation set accuracy and AUC\n",
    "            val_acc = None\n",
    "            val_auc = None\n",
    "            if validation_dataset is not None:\n",
    "                val_targets, val_preds = self.evaluate_val(validation_dataset)\n",
    "                val_targets = np.array(val_targets)\n",
    "                val_preds = np.array(val_preds)\n",
    "                val_auc = roc_auc_score(val_targets, val_preds)\n",
    "\n",
    "                val_preds[val_preds >= 0.5] = 1\n",
    "                val_preds[val_preds < 0.5] = 0\n",
    "                val_acc = accuracy_score(val_preds, val_targets)\n",
    "\n",
    "                e_val_accs.append(val_acc)\n",
    "                e_val_aucs.append(val_auc)\n",
    "            \n",
    "            e_bar.set_postfix(acc=acc, e_loss=e_losses[-1], auc=auc, val_acc=val_acc, val_auc=val_auc)\n",
    "      \n",
    "        return e_losses, e_accs, e_aucs, e_val_accs, e_val_aucs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train our RNN, we use our previously defined DataLoader and try to minimize error. The optimization function chosen is **Adamax** and the loss function is **Binary Cross-Entropy with Logits**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(input_dim=parameters['dynamic_dim'],\n",
    "            hidden_dim=parameters['hidden_dim'],\n",
    "            fixed_dim=parameters['fixed_dim'],\n",
    "            n_layers=parameters['n_layers'],\n",
    "            bi=parameters['bidirectional'],\n",
    "            use_gpu=parameters['use_gpu'],\n",
    "            dropout=parameters['dropout'],\n",
    "            lr=parameters['learning_rate'])\n",
    "\n",
    "if parameters['use_gpu']:\n",
    "    model.cuda()\n",
    "    \n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_losses, e_accs, e_aucs, e_val_accs, e_val_aucs = model.fit(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing results in pickles\n",
    "\n",
    "The results are stored in a dictionary with the following entries:\n",
    "- **parameters**: A dictionary of parameters for the given result\n",
    "- **losses**: The losses over time\n",
    "- **accs**: Accuracies over time for the training set\n",
    "- **aucs**: ROC AUC over time for the training set\n",
    "- **val_accs**: Accuracies over time for the validation set\n",
    "- **aucs**: ROC AUC over time for the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_pickle(dict_data, name):\n",
    "    pickle_out = open(RESULTS_DIR + name + '.pickle', 'wb')\n",
    "    pickle.dump(dict_data, pickle_out)\n",
    "    pickle_out.close()\n",
    "    \n",
    "data_to_store = {\n",
    "    'parameters': parameters,\n",
    "    'losses': e_losses,\n",
    "    'accs': e_accs,\n",
    "    'aucs': e_aucs,\n",
    "    'val_accs': e_val_accs,\n",
    "    'val_aucs': e_val_aucs\n",
    "}\n",
    "\n",
    "save_pickle(data_to_store, 'results_' + str(parameters['hidden_dim']) + '_' + str(parameters['n_layers']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
