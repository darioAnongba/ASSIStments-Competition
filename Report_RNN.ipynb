{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASSISTments Data Mining Competition 2017 - Optional Semester Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import WeightedRandomSampler, Sampler\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.init import xavier_normal, kaiming_normal\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_DIR = 'Data/'\n",
    "\n",
    "def init_seed(seed, use_gpu=False):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if use_gpu:\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "epochs = 10\n",
    "dropout = 0.25\n",
    "threshold = 20\n",
    "n_layers = 3\n",
    "hidden_dim = 256\n",
    "validation_set_size = 80\n",
    "balanced_data = False\n",
    "bidirectional = True\n",
    "use_gpu = True\n",
    "seed = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the training data\n",
    "\n",
    "The training data is stored in pickles as a dictionary where the keys are the student ids and and the value is tuple of **(sequence, fixed_features, target)**.\n",
    "\n",
    "The **sequence** is the sequence of actions of the specific student stored as a dataframe and sorted by time.\n",
    "The **fixed_feature** are the student features that are static through time like the average correctness, the school he's attending or his MCAS grade.\n",
    "The **target** is either yes (1) or no (0) the student has done a carrer in STEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Data/student_train_logs.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-6e7d9f58d1f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpickle_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"student_train_logs.pickle\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Data/student_train_logs.pickle'"
     ]
    }
   ],
   "source": [
    "pickle_train = open(DATA_DIR + \"student_train_logs.pickle\",\"rb\")\n",
    "train = pickle.load(pickle_train)\n",
    "\n",
    "train[9][0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dynamic_dim = train[9][0].shape[1]\n",
    "fixed_dim = train[9][1].shape[1]\n",
    "print(dynamic_dim, fixed_dim)\n",
    "input_dim = dynamic_dim + fixed_dim\n",
    "input_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing features that are out of range\n",
    "\n",
    "We discard columns that have a range that is too big outside [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns_discard = []\n",
    "\n",
    "for i in range(train[9][0].shape[1]):\n",
    "    v_min = 0\n",
    "    v_max = 1000000\n",
    "    for k, v in train.items():\n",
    "        name = v[0].columns[i]\n",
    "        v_min = min(v_min, v[0][name].min())\n",
    "        v_max = max(v_min, v[0][name].max())\n",
    "\n",
    "    if v_max - v_min > threshold:\n",
    "        columns_discard.append(name)\n",
    "\n",
    "print('Columns to discard with threshold:', threshold)\n",
    "print('Number of columns discarded:', len(columns_discard))\n",
    "input_dim = input_dim - len(columns_discard)\n",
    "columns_discard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k, v in train.items():\n",
    "    train[k] = (v[0].drop(columns_discard, axis=1), v[1], v[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a validation set\n",
    "\n",
    "We will simply take randomly 50 students as validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation = {k:v for k, v in random.sample(train.items(), validation_set_size)}\n",
    "train_truncated = { k : train[k] for k in set(train) - set(validation) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Truncating features out of range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Data Loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we implement data loaders that will enable us to iterate through our data and transform our data into tensors to be used with pyTorch.\n",
    "\n",
    "With PyTorch, every DataLoader can be set a sampler that will define how the data is being sampled. Here we implement a Random weighted sampler in order to traverse our data randomly and also be able to define if a class should be sampled more often than its appearance in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FullTrainingSet(Dataset):\n",
    "    def __init__(self, balance=False):\n",
    "        self.idx = list(train.keys())\n",
    "        self.sequences = train\n",
    "        \n",
    "        if balance:\n",
    "            self.weights = [0.3 if x[2] == 0 else 0.8 for x in self.sequences.values()]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, id):\n",
    "        student_id = self.idx[id]\n",
    "        \n",
    "        actions = self.sequences[student_id][0].as_matrix().astype(np.float32)\n",
    "        fixed = self.sequences[student_id][1].as_matrix().astype(np.float32)\n",
    "        target = np.asarray([self.sequences[student_id][2]]).astype(np.float32)\n",
    "        seq = np.hstack([fixed, actions])\n",
    "        \n",
    "        return student_id, seq, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TruncatedTrainingSet(Dataset):\n",
    "    def __init__(self, balance=False):\n",
    "        self.idx = list(train_truncated.keys())\n",
    "        self.sequences = train_truncated\n",
    "        \n",
    "        if balance:\n",
    "            self.weights = [0.3 if x[2] == 0 else 0.8 for x in self.sequences.values()]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, id):\n",
    "        student_id = self.idx[id]\n",
    "        \n",
    "        actions = self.sequences[student_id][0].as_matrix().astype(np.float32)\n",
    "        fixed = self.sequences[student_id][1].as_matrix().astype(np.float32)\n",
    "        target = np.asarray([self.sequences[student_id][2]]).astype(np.float32)\n",
    "        seq = np.hstack([fixed, actions])\n",
    "        \n",
    "        return student_id, seq, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ValidationSet(Dataset):\n",
    "    def __init__(self, balance=False):\n",
    "        self.idx = list(validation.keys())\n",
    "        self.sequences = validation\n",
    "        \n",
    "        if balance:\n",
    "            self.weights = [0.3 if x[2] == 0 else 0.8 for x in self.sequences.values()]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, id):\n",
    "        student_id = self.idx[id]\n",
    "        \n",
    "        actions = self.sequences[student_id][0].as_matrix().astype(np.float32)\n",
    "        fixed = self.sequences[student_id][1].as_matrix().astype(np.float32)\n",
    "        target = np.asarray([self.sequences[student_id][2]]).astype(np.float32)\n",
    "        seq = np.hstack([fixed, actions])\n",
    "        \n",
    "        return seq, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing that our sampler works fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_train_dataset = FullTrainingSet(balance=balanced_data)\n",
    "train_dataset = TruncatedTrainingSet(balance=balanced_data)\n",
    "validation_dataset = ValidationSet(balance=balanced_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building our RNN\n",
    "\n",
    "Our RNN is composed of an LSTM that takes as data our sequence of actions per student. The LSTM outputs a number of hidden parameters on which we append our fixed features. We then fave a fully connected layer that takes as input our concatenated features and outputs a value for each class. Finally, we use the sigmoid function to output a probability for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, bi, use_gpu=False, seed=7, dropout=0.25):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        init_seed(seed, use_gpu)\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.bi = bi\n",
    "        self.output_dim = output_dim\n",
    "        self.use_gpu = use_gpu\n",
    "        \n",
    "        self.gru = nn.GRU(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=n_layers,\n",
    "            dropout=dropout,\n",
    "            bidirectional=bi)\n",
    "        \n",
    "        if bi:\n",
    "            self.decoder = nn.Linear(hidden_dim*2, output_dim)\n",
    "        else:\n",
    "            self.decoder = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "            \n",
    "    def weights_init(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if param.dim() >= 2:\n",
    "                kaiming_normal(param)\n",
    "\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        if self.use_gpu:\n",
    "            if self.bi:\n",
    "                return Variable(torch.zeros(self.n_layers*2, batch_size, self.hidden_dim)).cuda()\n",
    "            else:\n",
    "                return Variable(torch.zeros(self.n_layers, batch_size, self.hidden_dim)).cuda()\n",
    "        else:\n",
    "            if self.bi:\n",
    "                return Variable(torch.zeros(self.n_layers*2, batch_size, self.hidden_dim))\n",
    "            else:\n",
    "                return Variable(torch.zeros(self.n_layers, batch_size, self.hidden_dim))\n",
    "\n",
    "        return Variable(torch.zeros(self.n_layers, batch_size, self.hidden_dim)).cuda()\n",
    "\n",
    "    \n",
    "    def forward(self, actions):\n",
    "        batch_size = actions.size(1)\n",
    "        hidden_state = self.init_hidden(batch_size)\n",
    "        out, _ = self.gru(actions, hidden_state)\n",
    "        out = out[-1,:,:]                                                                                                         \n",
    "        out = self.decoder(out)                                                                                                   \n",
    "        out = out.view(batch_size, self.output_dim)  \n",
    "    \n",
    "        return out\n",
    "\n",
    "\n",
    "    def step(self, inp, target):                                                                                                        \n",
    "        self.zero_grad()                                                                                                           \n",
    "        output = self.forward(inp)                                                                                                      \n",
    "        loss = self.criterion(output, target.float())                                                                      \n",
    "        loss.backward()                                                                                                                 \n",
    "        self.optimizer.step()          \n",
    "        \n",
    "        return loss.data[0], F.sigmoid(output)\n",
    "\n",
    "    \n",
    "    def evaluate_val(self, dataset, balanced=False):\n",
    "        if balanced:\n",
    "            sampler = WeightedRandomSampler(dataset.weights, num_samples=len(train_dataset))\n",
    "            loader = DataLoader(dataset, batch_size=1, num_workers=4, sampler=sampler)\n",
    "        else:\n",
    "            loader = DataLoader(dataset, batch_size=1, num_workers=4, shuffle=True)\n",
    "        \n",
    "        y_preds = []\n",
    "        y_true = []\n",
    "        \n",
    "        for i, (actions, target) in enumerate(tqdm_notebook(loader, leave=False)):\n",
    "            y_true.append(target.numpy()[0,0])\n",
    "            \n",
    "            actions = actions.permute(1, 0, 2)\n",
    "            \n",
    "            if self.use_gpu:\n",
    "                actions = Variable(actions).cuda()\n",
    "            else:\n",
    "                actions = Variable(actions)\n",
    "                \n",
    "            output = self.forward(actions)\n",
    "            output = F.sigmoid(output)\n",
    "            \n",
    "            if self.use_gpu:\n",
    "                y_preds.append(output.squeeze().cpu().data[0])\n",
    "            else:\n",
    "                y_preds.append(output.squeeze().data[0])\n",
    "                \n",
    "        return y_true, y_preds\n",
    "    \n",
    "    \n",
    "    def predict(self, test_set):\n",
    "        loader = DataLoader(test_set, batch_size=1, num_workers=4)\n",
    "        \n",
    "        preds = []\n",
    "        \n",
    "        for i, actions in enumerate(tqdm_notebook(loader, leave=False)):\n",
    "            actions = actions.permute(1, 0, 2)\n",
    "            \n",
    "            if self.use_gpu:\n",
    "                actions = Variable(actions).cuda()\n",
    "            else:\n",
    "                actions = Variable(actions)\n",
    "            \n",
    "            output = self.forward(actions)\n",
    "            output = F.sigmoid(output)\n",
    "            \n",
    "            if self.use_gpu:\n",
    "                preds.append(output.squeeze().cpu().data[0])\n",
    "            else:\n",
    "                preds.append(output.squeeze().data[0])\n",
    "                \n",
    "        return preds\n",
    "\n",
    "    \n",
    "    def fit(self, train_dataset, validation_dataset=None, epochs=10, balanced=False):\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "        self.optimizer = optim.Adamax(self.parameters(), lr=learning_rate)\n",
    "        \n",
    "        if balanced:\n",
    "            sampler = WeightedRandomSampler(train_dataset.weights, num_samples=len(train_dataset))\n",
    "            loader = DataLoader(train_dataset, batch_size=1, num_workers=4, sampler=sampler)\n",
    "        else:\n",
    "            loader = DataLoader(train_dataset, batch_size=1, num_workers=4, shuffle=True)\n",
    "\n",
    "        e_losses = []\n",
    "        e_accs = []\n",
    "        e_aucs = []\n",
    "    \n",
    "        e_val_accs = []\n",
    "        e_val_aucs = []\n",
    "        \n",
    "        e_bar = tqdm_notebook(range(epochs))\n",
    "        \n",
    "        for e in e_bar:\n",
    "            self.train()\n",
    "            e_loss = 0\n",
    "            preds = []\n",
    "            targets = []\n",
    "            val_preds = []\n",
    "            \n",
    "            for i, (_, seq, label) in enumerate(tqdm_notebook(loader, leave=False)):\n",
    "                seq = seq.permute(1, 0, 2)\n",
    "                \n",
    "                if self.use_gpu:\n",
    "                    seq_var = Variable(seq).cuda()\n",
    "                    label_var = Variable(label).cuda()\n",
    "                else:\n",
    "                    seq_var = Variable(seq)\n",
    "                    label_var = Variable(label)\n",
    "                    \n",
    "                loss, output = self.step(seq_var, label_var)\n",
    "                e_loss += loss\n",
    "                \n",
    "                if self.use_gpu:\n",
    "                    preds.append(output.squeeze().cpu().data[0])\n",
    "                else:\n",
    "                    preds.append(output.squeeze().data[0])\n",
    "                    \n",
    "                targets.append(label.numpy()[0,0])\n",
    "             \n",
    "            # Train set loss, accuracy and AUC\n",
    "            targets = np.array(targets)\n",
    "            preds = np.array(preds)\n",
    "            auc = roc_auc_score(targets, preds)\n",
    "            \n",
    "            preds[preds >= 0.5] = 1\n",
    "            preds[preds < 0.5] = 0\n",
    "            acc = accuracy_score(preds, targets)\n",
    "            \n",
    "            e_losses.append(e_loss / (i+1))\n",
    "            e_accs.append(acc)\n",
    "            e_aucs.append(auc)\n",
    "            \n",
    "            # Validation set accuracy and AUC\n",
    "            val_acc = None\n",
    "            val_auc = None\n",
    "            if validation_dataset is not None:\n",
    "                val_targets, val_preds = self.evaluate_val(validation_dataset, balanced=balanced)\n",
    "                val_targets = np.array(val_targets)\n",
    "                val_preds = np.array(val_preds)\n",
    "                val_auc = roc_auc_score(val_targets, val_preds)\n",
    "\n",
    "                val_preds[val_preds >= 0.5] = 1\n",
    "                val_preds[val_preds < 0.5] = 0\n",
    "                val_acc = accuracy_score(val_preds, val_targets)\n",
    "\n",
    "                e_val_accs.append(val_acc)\n",
    "                e_val_aucs.append(val_auc)\n",
    "            \n",
    "            e_bar.set_postfix(acc=acc, e_loss=e_losses[-1], auc=auc, val_acc=val_acc, val_auc=val_auc)\n",
    "\n",
    "        return e_losses, e_accs, e_aucs, e_val_accs, e_val_aucs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train our RNN, we use our previously defined DataLoader and try to minimize the Mean squared error using the MSELoss pyTorch loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = RNN(\n",
    "    input_dim=input_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    output_dim=1,\n",
    "    n_layers=n_layers,\n",
    "    bi=bidirectional,\n",
    "    use_gpu=use_gpu,\n",
    "    seed=seed,\n",
    "    dropout=dropout\n",
    ")\n",
    "model.weights_init()\n",
    "\n",
    "# If not using CUDA, comment this line\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "e_losses, e_accs, e_aucs, e_val_accs, e_val_aucs = model.fit(train_dataset, validation_dataset, epochs=epochs, balanced=balanced_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Training set loss, accuracy and AUC')\n",
    "plt.plot(e_losses)\n",
    "plt.plot(e_accs)\n",
    "plt.plot(e_aucs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Validation set accuracy and AUC')\n",
    "\n",
    "plt.plot(e_val_accs)\n",
    "plt.plot(e_val_aucs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_test = open(DATA_DIR + \"student_test_logs.pickle\",\"rb\")\n",
    "test = pickle.load(pickle_test)\n",
    "\n",
    "test[9][0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k, v in test.items():\n",
    "    test[k] = (v[0].drop(columns_discard, axis=1), v[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TestSet(Dataset):\n",
    "    def __init__(self):\n",
    "        self.idx = list(test.keys())\n",
    "        self.sequences = test\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, id):\n",
    "        student_id = self.idx[id]\n",
    "        \n",
    "        actions = self.sequences[student_id][0].as_matrix().astype(np.float32)\n",
    "        fixed = self.sequences[student_id][1].as_matrix().astype(np.float32)\n",
    "        seq = np.hstack([fixed, actions])\n",
    "        \n",
    "        return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_set = TestSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.predict(test_set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
