{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASSISTments Data Mining Competition 2017 - Optional Semester Project\n",
    "\n",
    "## Data Preprocessing\n",
    "\n",
    "The purpose of this notebook is to explain and handle the data preprocessing needed to fuel the Deep Learning Model used for this project. The Model can be found in the \"`Report`\" notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_DIR = 'Data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is composed of multiple types of features that we need to differentiate. The types are the following:\n",
    "\n",
    "- **Floating averages**: Values in the range [0, 1] representing averages. *These values can be used without transformation*. Ex: `AveCorrect`\n",
    "- **Categorical**: Values representing categories or indexes. These values need to be treated carefully because a value of \"1184832848\" represents an index and not a numerical value. This means that *we need to transform these values in order to use them in a neural network using embeddings*. Ex: `assignmentId`\n",
    "- **Numerical**: Values representing integers. *These values need to be scaled* to a range (like [0, 1] or [-1, 1]) in order to be used in a neural network. Ex: `timeTaken`\n",
    "- **Binary**: Values representing booleans. These values can be used without *transformation*. Ex: `correct`\n",
    "\n",
    "In addition, some values are discarded because of missing values, non-usefulness (their information is already contained in another feature) or uniqueness (`actionId` is different for each action so there is no information to extract from that feature)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "column_names = set(['AveCarelessness', 'AveCorrect', 'AveKnow', 'AveResBored', 'AveResConf',\n",
    "       'AveResEngcon', 'AveResFrust', 'AveResGaming', 'AveResOfftask',\n",
    "       'ITEST_id', 'Ln', 'Ln-1', 'NumActions', 'Prev5count', 'RES_BORED',\n",
    "       'RES_CONCENTRATING', 'RES_CONFUSED', 'RES_FRUSTRATED', 'RES_GAMING',\n",
    "       'RES_OFFTASK', 'SY ASSISTments Usage', 'actionId', 'assignmentId',\n",
    "       'assistmentId', 'attemptCount', 'bottomHint', 'confidence(BORED)',\n",
    "       'confidence(CONCENTRATING)', 'confidence(CONFUSED)',\n",
    "       'confidence(FRUSTRATED)', 'confidence(GAMING)', 'confidence(OFF TASK)',\n",
    "       'consecutiveErrorsInRow', 'correct', 'endTime',\n",
    "       'endsWithAutoScaffolding', 'endsWithScaffolding', 'frIsHelpRequest',\n",
    "       'frIsHelpRequestScaffolding', 'frPast5HelpRequest', 'frPast5WrongCount',\n",
    "       'frPast8HelpRequest', 'frPast8WrongCount', 'frTimeTakenOnScaffolding',\n",
    "       'frTotalSkillOpportunitiesScaffolding', 'frWorkingInSchool',\n",
    "       'helpAccessUnder2Sec', 'hint', 'hintCount', 'hintTotal', 'manywrong',\n",
    "       'original', 'past8BottomOut', 'prev5count', 'problemId', 'problemType',\n",
    "       'responseIsChosen', 'responseIsFillIn', 'scaffold', 'skill',\n",
    "       'startTime', 'stlHintUsed', 'sumRight', 'sumTime3SDWhen3RowRight',\n",
    "       'sumTimePerSkill', 'timeGreater10SecAndNextActionRight',\n",
    "       'timeGreater5Secprev2wrong', 'timeOver80', 'timeSinceSkill',\n",
    "       'timeTaken', 'totalFrAttempted', 'totalFrPastWrongCount',\n",
    "       'totalFrPercentPastWrong', 'totalFrSkillOpportunities',\n",
    "       'totalFrSkillOpportunitiesByScaffolding', 'totalFrTimeOnSkill',\n",
    "       'totalTimeByPercentCorrectForskill'])\n",
    "\n",
    "categorical_features = set(['skill',\n",
    "                        'problemId',\n",
    "                        'assignmentId',\n",
    "                        'assistmentId',\n",
    "                        'problemType'])\n",
    "\n",
    "numerical_features = set(['NumActions',\n",
    "                      'attemptCount',\n",
    "                      'consecutiveErrorsInRow',\n",
    "                      'frPast5HelpRequest',\n",
    "                      'frPast5WrongCount',\n",
    "                      'frPast8HelpRequest',\n",
    "                      'frPast8WrongCount',\n",
    "                      'frTimeTakenOnScaffolding',\n",
    "                      'frTotalSkillOpportunitiesScaffolding',\n",
    "                      'hintCount',\n",
    "                      'hintTotal',\n",
    "                      'past8BottomOut',\n",
    "                      'sumRight',\n",
    "                      'sumTimePerSkill',\n",
    "                      'timeSinceSkill',\n",
    "                      'timeTaken',\n",
    "                      'totalFrAttempted',\n",
    "                      'totalFrPastWrongCount',\n",
    "                      'totalFrSkillOpportunities',\n",
    "                      'totalFrSkillOpportunitiesByScaffolding',\n",
    "                      'totalFrTimeOnSkill',\n",
    "                      'totalTimeByPercentCorrectForskill'])\n",
    "\n",
    "columns_not_keep = set([\n",
    "    'SY ASSISTments Usage',\n",
    "    'Prev5count',\n",
    "    'prev5count',\n",
    "    'endTime',\n",
    "    'responseIsChosen',\n",
    "    'sumTime3SDWhen3RowRight',\n",
    "    'Ln',\n",
    "    'Ln-1',\n",
    "    'actionId'])\n",
    "\n",
    "columns_keep = column_names - columns_not_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_logs = pd.concat([\n",
    "    pd.read_csv(DATA_DIR + 'student_log_' + str(i) + '.csv', usecols=columns_keep) for i in range(1, 11)\n",
    "], ignore_index=True)\n",
    "\n",
    "student_logs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Na\n",
    "\n",
    "We remove actions containing NA values (normally, there shouldn't be any)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(student_logs.isnull().any().any())\n",
    "student_logs = student_logs.fillna(0)\n",
    "print(student_logs.isnull().any().any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling numerical features\n",
    "\n",
    "We need to scale numerical values using different scaling methods:\n",
    "- **StandardScaler**: Standardize features by removing the mean and scaling to unit variance.\n",
    "- **MaxMinScaler**: Transforms features by scaling each feature to a given range.\n",
    "\n",
    "After testing with both methods, we chose a MaxMinScaler with range [-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler((-1, 1))\n",
    "numerical_features = list(numerical_features)\n",
    "student_logs[numerical_features] = scaler.fit_transform(student_logs[numerical_features])\n",
    "student_logs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding categorical features\n",
    "\n",
    "We need to encode categorical features into integer indexes. There are two types of categorical features:\n",
    "- **Indexes**: Integers used to define a feature of an action like the problem id or the assignment id (`assignmentId`)\n",
    "- **Categories**: Strings used to state that an action is part of a given category, like the skill being tested (`skill`)\n",
    "\n",
    "With both types the problem of label encoding is the same as we need to transform the values into integers spanning from 0 to the amount of different categories for that feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MultiColumnLabelEncoder:\n",
    "    def __init__(self,columns = None):\n",
    "        self.columns = columns # array of column names to encode\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        return self # not relevant here\n",
    "\n",
    "    def transform(self,X):\n",
    "        '''\n",
    "        Transforms columns of X specified in self.columns using\n",
    "        LabelEncoder(). If no columns specified, transforms all\n",
    "        columns in X.\n",
    "        '''\n",
    "        output = X.copy()\n",
    "        if self.columns is not None:\n",
    "            for col in self.columns:\n",
    "                output[col] = LabelEncoder().fit_transform(output[col])\n",
    "        else:\n",
    "            for colname,col in output.iteritems():\n",
    "                output[colname] = LabelEncoder().fit_transform(col)\n",
    "        return output\n",
    "\n",
    "    def fit_transform(self,X,y=None):\n",
    "        return self.fit(X,y).transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_logs_categorical = MultiColumnLabelEncoder(columns = categorical_features).fit_transform(student_logs)\n",
    "student_logs_categorical.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_logs_categorical['skill'].sort_values().unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating embeddings for categorical features\n",
    "\n",
    "Now that each category has been encoded into an integer index, we can apply vector embeddings methods to transform each value into a X dimensional vector. We chose vectors of size 3.\n",
    "\n",
    "We also need to drop the \"old\" column containing the indexes and only keep the new embedding features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_embeddings = 3\n",
    "\n",
    "for column in categorical_features:\n",
    "    categories_idx = []\n",
    "    embeddings = []\n",
    "\n",
    "    for idx in student_logs_categorical[column]:\n",
    "        categories_idx.append(idx.item())\n",
    "\n",
    "    embeds = nn.Embedding(len(categories_idx), dim_embeddings) # 3 dimensional embeddings\n",
    "    lookup_tensor = torch.LongTensor(np.asarray(categories_idx))\n",
    "    embed = embeds(Variable(lookup_tensor))\n",
    "\n",
    "    df = pd.DataFrame(embed.data.numpy()).rename(columns={'0': column+'_0', '1': column+'_1', '2':column+'_2'})\n",
    "    df.columns = [column+'_0', column+'_1', column+'_2']\n",
    "    student_logs_categorical = pd.concat([student_logs_categorical, df], axis=1)\n",
    "\n",
    "student_logs_categorical.drop(categorical_features, axis=1, inplace=True)\n",
    "student_logs_categorical.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keeping only students for whom we have labels\n",
    "\n",
    "Some students do not have any labels assigned so they are useless for supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = pd.read_csv('Data/training_label.csv', index_col='ITEST_id', na_values=-999).sort_index()\n",
    "train_labels.drop_duplicates(subset=None, keep='first', inplace=True)\n",
    "\n",
    "train_labels = train_labels.fillna(train_labels['MCAS'].median())\n",
    "train_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = pd.read_csv(DATA_DIR + 'validation_test_label.csv', index_col='ITEST_id', na_values=-999).sort_index()\n",
    "test_labels = test_labels.fillna(train_labels['MCAS'].median())\n",
    "test_labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only keep actions for students for which we have labels in train_labels and test_labels. We also sort by student ID and by startTime of in order to have a chronological suite of actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "student_logs_categorical = student_logs_categorical.sort_values(by=['ITEST_id', 'startTime'])\n",
    "del student_logs_categorical['startTime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = train_labels.index.values\n",
    "test_idx = test_labels.index.values\n",
    "\n",
    "student_train_logs = student_logs_categorical[student_logs_categorical['ITEST_id'].isin(train_idx)]\n",
    "student_test_logs = student_logs_categorical[student_logs_categorical['ITEST_id'].isin(test_idx)]\n",
    "print('Training data shape:', student_train_logs.shape)\n",
    "print('Test data shape:', student_test_logs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of students train:', student_train_logs.ITEST_id.unique().shape)\n",
    "print('Number of students test:', student_test_logs.ITEST_id.unique().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_train_logs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a dictionary of sequences \n",
    "\n",
    "Instead of storing all action sequences into a single dataframe, we separate actions par student and create a dictionary from student id to a sequence of actions. To be exact, the values of the dictionary are arrays of size 3 containing:\n",
    "- **Sequence of dynamic features**: Features that are different for every student action. The result is a pandas dataframe.\n",
    "- **Sequence of static features**: Features that stay the same for every action of a student (averages, school id and MCAS). The result is a Pandas Series.\n",
    "- **label**: If yes or no the student has chosen a STEM career. The result is a boolean (0 or 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_features = ['NumActions',\n",
    "                  'AveKnow',\n",
    "                  'AveCarelessness',\n",
    "                  'AveCorrect',\n",
    "                  'AveResBored',\n",
    "                  'AveResEngcon',\n",
    "                  'AveResConf',\n",
    "                  'AveResFrust',\n",
    "                  'AveResOfftask',\n",
    "                  'AveResGaming']\n",
    "\n",
    "def create_dict(idx, labels, is_train=True):\n",
    "    dict_data = {}\n",
    "\n",
    "    for i in idx:\n",
    "        sequence = student_logs_categorical[student_logs_categorical['ITEST_id'] == i]\n",
    "        sequence = sequence.drop(['ITEST_id'], axis=1)\n",
    "        fixed = sequence[fixed_features]\n",
    "        fixed = fixed.assign(MCAS=labels.loc[i].MCAS, SchoolId=labels.loc[i].SchoolId).iloc[0]\n",
    "        sequence = sequence.drop(fixed_features, axis=1)\n",
    "        \n",
    "        if is_train:\n",
    "            target = train_labels.loc[i].isSTEM\n",
    "            dict_data[i] = (sequence, fixed, target)\n",
    "        else:\n",
    "            dict_data[i] = (sequence, fixed)\n",
    "        \n",
    "    return dict_data\n",
    "\n",
    "dict_train = create_dict(train_idx, train_labels)\n",
    "dict_test = create_dict(test_idx, test_labels, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dict_train))\n",
    "print(len(dict_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving data in pickles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we save the data into pickles to use them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_pickle(dict_data, name):\n",
    "    pickle_out = open(DATA_DIR + name + '.pickle', 'wb')\n",
    "    pickle.dump(dict_data, pickle_out)\n",
    "    pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_pickle(dict_train, 'student_train_logs')\n",
    "save_pickle(dict_test, 'student_test_logs')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
